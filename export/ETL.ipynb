{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f636565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerar_consolidados_sem_hash_e_sem_upload.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f485c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulo especifico para rodar o notebook fora da raiz do projeto\n",
    "# Garante que a raiz do projeto (onde está a pasta utils/) entre no sys.path\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) \\\n",
    "    if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "\n",
    "# Adiciona pasta raiz ao sys.path\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52618b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== PARTE 1: Séries (consolidado.parquet)\n",
    "# =========================\n",
    "\n",
    "def carregar_dados(arquivo, aba, colunas, horas=None):\n",
    "    \"\"\"\n",
    "    Lê uma aba do Excel com header=4 e apenas as colunas indicadas.\n",
    "    Renomeia colunas para ['Data', HH:MM, HH:MM, ...].\n",
    "    Preenche datas faltantes somando +1 dia quando a linha anterior tem Data.\n",
    "    Mantém exatamente a mesma lógica do script original.\n",
    "    \"\"\"\n",
    "    dados = pd.read_excel(arquivo, sheet_name=aba, header=4, usecols=colunas)\n",
    "    nomes_colunas = [\"Data\"] + (horas if horas else [f\"{str(h).zfill(2)}:00\" for h in range(1, 24)] + [\"24:00\"])\n",
    "    dados.columns = nomes_colunas\n",
    "    for i in range(len(dados)):\n",
    "        if pd.isna(dados.loc[i, \"Data\"]) and i > 0 and pd.notna(dados.loc[i-1, \"Data\"]):\n",
    "            dados.loc[i, \"Data\"] = dados.loc[i-1, \"Data\"] + pd.Timedelta(days=1)\n",
    "    return dados.dropna(subset=[\"Data\"]).dropna(how=\"all\")\n",
    "\n",
    "def processar_dados(dados, valor_maximo, nome_fonte):\n",
    "    \"\"\"\n",
    "    Varre linhas/horas, limpa valores, filtra por limites e monta:\n",
    "    ['Fonte','DataHoraReal','Valor','MediaMovel_6'].\n",
    "    Mantém a MM de janela 6 exatamente como estava (sem groupby/ordenar antes).\n",
    "    \"\"\"\n",
    "    linhas = []\n",
    "    for _, row in dados.iterrows():\n",
    "        data_atual = row[\"Data\"]\n",
    "        for coluna in row.index:\n",
    "            if coluna != \"Data\":\n",
    "                valor_bruto = row[coluna]\n",
    "                if isinstance(valor_bruto, str):\n",
    "                    valor_bruto = valor_bruto.replace(\"<\", \"\").replace(\",\", \".\").strip()\n",
    "                valor = pd.to_numeric(valor_bruto, errors=\"coerce\")\n",
    "                if pd.notna(valor) and valor != 0 and valor <= valor_maximo:\n",
    "                    linhas.append({\n",
    "                        \"Data\": data_atual,\n",
    "                        \"Hora\": coluna,\n",
    "                        \"Valor\": valor,\n",
    "                        \"Fonte\": nome_fonte\n",
    "                    })\n",
    "    df = pd.DataFrame(linhas)\n",
    "    if not df.empty:\n",
    "        df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\").dt.date\n",
    "        df[\"HoraCorrigida\"] = df[\"Hora\"].replace({\"24:00\": \"23:59\"})\n",
    "        df[\"DataHoraReal\"] = pd.to_datetime(df[\"Data\"].astype(str) + \" \" + df[\"HoraCorrigida\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"DataHoraReal\", \"Valor\"])\n",
    "        df = df[df[\"Valor\"] <= valor_maximo].reset_index(drop=True)\n",
    "        df[\"MediaMovel_6\"] = df[\"Valor\"].rolling(window=6, min_periods=1).mean()\n",
    "        df = df[[\"Fonte\", \"DataHoraReal\", \"Valor\", \"MediaMovel_6\"]]\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# ====== PARTE 2: Batelada (consolidado_batelada.parquet)\n",
    "# =========================\n",
    "\n",
    "def carregar_dados_batelada(arquivo, aba, colunas):\n",
    "    \"\"\"\n",
    "    Lê aba com header=4, zera nomes das colunas (0..N-1), seleciona posições em 'colunas',\n",
    "    renomeia para ['Data','Batelada','Hora','ValorBruto'] e preenche Data ausente (+1 dia).\n",
    "    \"\"\"\n",
    "    dados = pd.read_excel(arquivo, sheet_name=aba, header=4)\n",
    "    dados.columns = list(range(dados.shape[1]))\n",
    "    df = dados[colunas].copy()\n",
    "    df.columns = [\"Data\", \"Batelada\", \"Hora\", \"ValorBruto\"]\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if pd.isna(df.at[i, \"Data\"]) and pd.notna(df.at[i - 1, \"Data\"]):\n",
    "            try:\n",
    "                df.at[i, \"Data\"] = df.at[i - 1, \"Data\"] + pd.Timedelta(days=1)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return df.dropna(subset=[\"Data\", \"Hora\", \"Batelada\", \"ValorBruto\"]).dropna(how=\"all\")\n",
    "\n",
    "def processar_dados_batelada(dados, valor_maximo, nome_fonte):\n",
    "    \"\"\"\n",
    "    Normaliza ValorBruto, filtra e monta:\n",
    "    ['DataHoraReal','Valor','Batelada','Fonte'].\n",
    "    Mantém exatamente a mesma lógica do script original.\n",
    "    \"\"\"\n",
    "    dados[\"Valor\"] = (\n",
    "        dados[\"ValorBruto\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[^\\d,.\\-]\", \"\", regex=True)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "        .str.replace(r\"\\.{2,}\", \".\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    dados[\"Valor\"] = pd.to_numeric(dados[\"Valor\"], errors=\"coerce\")\n",
    "    dados = dados[(dados[\"Valor\"].notna()) & (dados[\"Valor\"] != 0) & (dados[\"Valor\"] <= valor_maximo)].copy()\n",
    "\n",
    "    if dados.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dados[\"Data\"] = pd.to_datetime(dados[\"Data\"], errors=\"coerce\").dt.date\n",
    "    dados[\"HoraCorrigida\"] = dados[\"Hora\"].astype(str).str.strip().replace({\"24:00\": \"23:59\"})\n",
    "    dados[\"DataHoraReal\"] = pd.to_datetime(\n",
    "        dados[\"Data\"].astype(str) + \" \" + dados[\"HoraCorrigida\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    dados[\"Batelada\"] = pd.to_numeric(dados[\"Batelada\"], errors=\"coerce\")\n",
    "    dados = dados[dados[\"Batelada\"].notna()]\n",
    "    dados = dados[dados[\"Batelada\"] % 1 == 0]\n",
    "    dados[\"Batelada\"] = dados[\"Batelada\"].astype(\"int64\")\n",
    "    dados = dados.dropna(subset=[\"DataHoraReal\", \"Valor\", \"Batelada\"])\n",
    "    dados[\"Fonte\"] = nome_fonte\n",
    "\n",
    "    return dados[[\"DataHoraReal\", \"Valor\", \"Batelada\", \"Fonte\"]]\n",
    "\n",
    "# =========================\n",
    "# ====== EXECUÇÃO (sem hash e sem upload)\n",
    "# =========================\n",
    "\n",
    "def baixar_excel_para_bytesio(fonte_excel):\n",
    "    \"\"\"\n",
    "    Se 'fonte_excel' for URL (http/https), baixa via requests.\n",
    "    Se for caminho local (.xlsx), abre direto.\n",
    "    Retorna um objeto BytesIO ou o próprio caminho (ambos são aceitos por read_excel).\n",
    "    \"\"\"\n",
    "    if isinstance(fonte_excel, str) and fonte_excel.lower().startswith((\"http://\", \"https://\")):\n",
    "        print(\"Baixando arquivo do SharePoint/URL...\")\n",
    "        resp = requests.get(fonte_excel)\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(f\"Erro ao baixar o arquivo (status {resp.status_code}).\")\n",
    "        return BytesIO(resp.content)\n",
    "    # caminho local:\n",
    "    return fonte_excel\n",
    "\n",
    "def gerar_consolidados(\n",
    "    fonte_excel,\n",
    "    conjuntos_series=None,\n",
    "    conjuntos_batelada=None,\n",
    "    caminho_series=\"consolidado.parquet\",\n",
    "    caminho_batelada=\"consolidado_batelada.parquet\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Executa os dois pipelines (séries e batelada) SEM hash e SEM upload.\n",
    "    Salva os arquivos parquet nos caminhos informados.\n",
    "    Retorna (df_final, df_final_batelada).\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    fonte_excel : str ou bytes-like\n",
    "        Caminho ou fonte do Excel.\n",
    "    conjuntos_series : iterable[tuple]\n",
    "        Cada tupla: (aba, colunas, val_max, nome, horas, [filtro])\n",
    "    conjuntos_batelada : iterable[tuple]\n",
    "        Cada tupla: (aba, colunas, val_max, nome, [filtro])\n",
    "    \"\"\"\n",
    "\n",
    "    if conjuntos_series is None:\n",
    "        conjuntos_series = CONJUNTOS_SERIES_DEFAULT\n",
    "\n",
    "    if conjuntos_batelada is None:\n",
    "        conjuntos_batelada = CONJUNTOS_BATELADA_DEFAULT\n",
    "\n",
    "    excel_data = baixar_excel_para_bytesio(fonte_excel)\n",
    "\n",
    "    # =========================\n",
    "    #        SÉRIES\n",
    "    # =========================\n",
    "    todos_dados = []\n",
    "    print(\"Processando dados (séries)...\")\n",
    "\n",
    "    for item in conjuntos_series:\n",
    "        # Suporta tanto (aba, colunas, val_max, nome, horas)\n",
    "        # quanto (aba, colunas, val_max, nome, horas, filtro)\n",
    "        if len(item) == 5:\n",
    "            aba, colunas, val_max, nome, horas = item\n",
    "            filtro = None\n",
    "        else:\n",
    "            aba, colunas, val_max, nome, horas, filtro = item\n",
    "\n",
    "        dados = carregar_dados(excel_data, aba, colunas, horas)\n",
    "        df = processar_dados(dados, val_max, nome)\n",
    "\n",
    "        if not df.empty:\n",
    "            df[\"Filtro\"] = filtro\n",
    "            todos_dados.append(df)\n",
    "\n",
    "    if todos_dados:\n",
    "        df_final = pd.concat(todos_dados, ignore_index=True)\n",
    "    else:\n",
    "        df_final = pd.DataFrame(\n",
    "            columns=[\"Fonte\", \"DataHoraReal\", \"Valor\", \"MediaMovel_6\", \"Filtro\"]\n",
    "        )\n",
    "\n",
    "    df_final = df_final.sort_values(by=\"DataHoraReal\", ascending=False).reset_index(drop=True)\n",
    "    print(f\"Séries consolidadas: {len(df_final)} linhas\")\n",
    "    df_final.to_parquet(caminho_series, index=False)\n",
    "    print(f\"Arquivo salvo: {caminho_series}\")\n",
    "\n",
    "    # =========================\n",
    "    #       BATELADA\n",
    "    # =========================\n",
    "    todos_batelada = []\n",
    "    print(\"Processando dados de batelada...\")\n",
    "\n",
    "    for item in conjuntos_batelada:\n",
    "        # Suporta tanto (aba, colunas, val_max, nome)\n",
    "        # quanto (aba, colunas, val_max, nome, filtro)\n",
    "        if len(item) == 4:\n",
    "            aba, colunas, val_max, nome = item\n",
    "            filtro = None\n",
    "        else:\n",
    "            aba, colunas, val_max, nome, filtro = item\n",
    "\n",
    "        dados_b = carregar_dados_batelada(excel_data, aba, colunas)\n",
    "        df_b = processar_dados_batelada(dados_b, val_max, nome)\n",
    "        print(f\"{nome}: {len(df_b)} linhas processadas\")\n",
    "\n",
    "        if not df_b.empty:\n",
    "            df_b[\"Filtro\"] = filtro\n",
    "            todos_batelada.append(df_b)\n",
    "\n",
    "    if todos_batelada:\n",
    "        df_final_batelada = (\n",
    "            pd.concat(todos_batelada, ignore_index=True)\n",
    "            .drop_duplicates(subset=[\"Fonte\", \"DataHoraReal\", \"Valor\", \"Batelada\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        df_final_batelada = pd.DataFrame(\n",
    "            columns=[\"DataHoraReal\", \"Valor\", \"Batelada\", \"Fonte\", \"Filtro\"]\n",
    "        )\n",
    "\n",
    "    df_final_batelada = df_final_batelada.sort_values(by=\"DataHoraReal\", ascending=False)\n",
    "    df_final_batelada[\"Valor\"] = pd.to_numeric(df_final_batelada[\"Valor\"], errors=\"coerce\")\n",
    "    if not df_final_batelada.empty:\n",
    "        df_final_batelada[\"Batelada\"] = df_final_batelada[\"Batelada\"].astype(\"int64\")\n",
    "\n",
    "    df_final_batelada.to_parquet(\n",
    "        caminho_batelada,\n",
    "        index=False,\n",
    "        engine=\"pyarrow\",\n",
    "        compression=\"snappy\",\n",
    "    )\n",
    "    print(f\"Arquivo salvo: {caminho_batelada}\")\n",
    "\n",
    "    return df_final, df_final_batelada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd028d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Configurações de horários -----\n",
    "HORARIOS_3 = [\"08:00\", \"16:00\", \"24:00\"]\n",
    "HORARIOS_4 = [\"06:00\", \"12:00\", \"18:00\", \"24:00\"]\n",
    "HORARIOS_6 = [\"04:00\", \"08:00\", \"12:00\", \"16:00\", \"20:00\", \"24:00\"]\n",
    "HORARIOS_2 = [\"12:00\", \"24:00\"]\n",
    "HORARIOS_12 = [\"02:00\", \"04:00\", \"06:00\", \"08:00\", \"10:00\", \"12:00\",\"14:00\", \"16:00\", \"18:00\", \"20:00\", \"22:00\", \"24:00\"]\n",
    "HORARIOS_24 = [\"01:00\", \"02:00\", \"03:00\", \"04:00\", \"05:00\",\"06:00\", \"07:00\", \"08:00\", \"09:00\", \"10:00\", \"11:00\",\"12:00\",\n",
    "               \"13:00\", \"14:00\", \"15:00\", \"16:00\", \"17:00\",\"18:00\", \"19:00\", \"20:00\", \"21:00\", \"22:00\", \"23:00\",\"24:00\"\n",
    "]\n",
    "HORARIOS_BAR = [\"04:00\", \"08:00\", \"12:00\", \"16:00\", \"20:00\", \"24:00\"]\n",
    "\n",
    "# ----- Conjuntos padrão de séries -----\n",
    "# Estrutura: (aba, colunas, val_max, nome, horas)\n",
    "CONJUNTOS_SERIES_DEFAULT = [\n",
    "    # Sólidas\n",
    "    (\"Sólidas\", [0, 30, 35, 40], 50,  \"LIX_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 45, 59],     50,  \"LIX_Au_S\", HORARIOS_2,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 27, 32, 37, 42], 50,  \"LIX_Au_S\", HORARIOS_4,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 47, 49, 51, 53, 55, 57], 50, \"LIX_Au_S\", HORARIOS_6,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 31, 36, 41], 200, \"LIX_PX\",    HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 46, 61],     200, \"LIX_PX\",    HORARIOS_2,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 48, 50, 52, 54, 56, 58], 200, \"LIX_PX\", HORARIOS_6,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 76, 77, 78], 50,  \"REJ_Au_S\",  HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas\", [0, 72, 74],     50,  \"REJ_Au_S\",  HORARIOS_2,\"solidas\"),\n",
    "    (\"Sólidas Saída TQ02, TQ05 e TQ06\", [0, 7, 8, 9],  50, \"TQ2_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas Saída TQ02, TQ05 e TQ06\", [0, 14, 15, 16], 50, \"TQ5_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas Saída TQ02, TQ05 e TQ06\", [0, 21, 22, 23], 50, \"TQ6_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas Saída TQ02, TQ05 e TQ06\", [0, 28, 29, 30], 50, \"TQ9_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas Saída TQ02, TQ05 e TQ06\", [0, 35, 36, 37], 50, \"TQ10_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas Saída TQ02, TQ05 e TQ06\", [0, 42, 43, 44], 50, \"TQ11_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Sólidas Saída TQ02, TQ05 e TQ06\", [0, 49, 50, 51], 50, \"TQ12_Au_S\", HORARIOS_3,\"solidas\"),\n",
    "    (\"Carvão TQ Produção\", [0, 8], 50, \"TQ2_Au_S\", [\"12:00\"],\"solidas\"),\n",
    "\n",
    "    # Líquidas\n",
    "    (\"Água de Processo\", [0, 15, 16, 17, 18, 19, 20], 0.6, \"BAR_Au_L\",  HORARIOS_BAR, \"liquidas\"),\n",
    "    (\"Líquidas\",         [0, 38, 39, 40],            50,   \"LIX_Au_L\",  HORARIOS_3, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0] + list(range(7, 31)), 5, \"TQ01_Au_L\", HORARIOS_24, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0, 32, 33, 34], 1.5, \"TQ02_Au_L\", HORARIOS_3, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0, 51, 52, 53], 50,  \"TQ06_Au_L\", HORARIOS_3, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0] + list(range(82, 94)), 50, \"TQ07_Au_L\", HORARIOS_12, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0] + list(range(111, 123)), 50, \"TQ09_Au_L\", HORARIOS_12, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0] + list(range(136, 148)), 50, \"TQ10_Au_L\", HORARIOS_12, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0] + list(range(161, 173)), 50, \"TQ11_Au_L\", HORARIOS_12, \"liquidas\"),\n",
    "    (\"Líquidas Saída TQ1 TQ2 TQ6 TQ7\", [0] + list(range(186, 198)), 50, \"TQ12_Au_L\", HORARIOS_12, \"liquidas\"),\n",
    "    (\"Líquidas\", [0, 101, 102, 103], 0.8, \"REJ_Au_L\", HORARIOS_3, \"liquidas\"),\n",
    "]\n",
    "\n",
    "# ----- Conjuntos padrão de batelada -----\n",
    "# Estrutura: (aba, colunas, val_max, nome)\n",
    "CONJUNTOS_BATELADA_DEFAULT = [\n",
    "    (\"Cuba Principal\",    [1, 4, 3, 5],   500,  \"CUBA_Entrada_Au\",\"eluicao\"),\n",
    "    (\"Cuba Principal\",    [1, 4, 3, 6],   500,  \"CUBA_Entrada_NaOH\",\"eluicao\"),\n",
    "    (\"Cuba Principal\",    [1, 4, 3, 7],   500,  \"CUBA_Entrada_CN\",\"eluicao\"),\n",
    "    (\"Cuba Principal\",    [9, 12, 11, 13], 500, \"CUBA_Saida_Au\",\"eluicao\"),\n",
    "    (\"Cuba Principal\",    [9, 12, 11, 51], 500, \"CUBA_Saida_NaOH\",\"eluicao\"),\n",
    "    (\"Cuba Principal\",    [9, 12, 11, 52], 500, \"CUBA_Saida_CN\",\"eluicao\"),\n",
    "    (\"Acacia\",  [1, 4, 2, 5],  5000,  \"ACA_Rica\",\"acacia\"),\n",
    "    (\"Acacia\",  [1, 4, 2, 11], 5000,  \"ACA_Pobre\",\"acacia\"),\n",
    "    (\"Acacia\",  [1, 4, 2, 7],  5000,  \"ACA_CN\",\"acacia\"),\n",
    "    (\"Eluição - Carvão\", [2, 1, 3, 4],  5000, \"ELU_Rica\",\"eluicao\"),\n",
    "    (\"Eluição - Carvão\", [6, 1, 7, 8],  5000, \"ELU_Pobre\",\"eluicao\"),\n",
    "    (\"Eluição - Carvão\", [6, 1, 7, 11], 5000, \"ELU_ATV\",\"eluicao\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d3e7405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dados (séries)...\n",
      "Séries consolidadas: 32996 linhas\n",
      "Arquivo salvo: ../export/amostras_horarias.parquet\n",
      "Processando dados de batelada...\n",
      "CUBA_Entrada_Au: 6740 linhas processadas\n",
      "CUBA_Entrada_NaOH: 2322 linhas processadas\n",
      "CUBA_Entrada_CN: 2413 linhas processadas\n",
      "CUBA_Saida_Au: 6695 linhas processadas\n",
      "CUBA_Saida_NaOH: 2982 linhas processadas\n",
      "CUBA_Saida_CN: 3076 linhas processadas\n",
      "ACA_Rica: 2114 linhas processadas\n",
      "ACA_Pobre: 2006 linhas processadas\n",
      "ACA_CN: 2109 linhas processadas\n",
      "ELU_Rica: 1477 linhas processadas\n",
      "ELU_Pobre: 1434 linhas processadas\n",
      "ELU_ATV: 408 linhas processadas\n",
      "Arquivo salvo: ../export/amostras_bateladas.parquet\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo de excel\n",
    "URL_EXCEL = r\"C:\\Users\\Dataminds2\\Aura Minerals\\Almas - Performance - Data Minds - Data Minds\\09 - Automações\\Arquivos_Onedrive\\Resultados Planta.xlsx\"\n",
    "\n",
    "# Execução principal\n",
    "df_amostras, df_batelada = gerar_consolidados(\n",
    "    fonte_excel=URL_EXCEL,\n",
    "    caminho_series=\"../export/amostras_horarias.parquet\",\n",
    "    caminho_batelada=\"../export/amostras_bateladas.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
